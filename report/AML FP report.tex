\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algpseudocode,algorithm}
\usepackage{multicol}

\title{Implementing GAN on MNIST and SVHN}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Shengjie Sun \\
  ss5593\\
  Department of Statistics\\
  Columbia University\\
  New York, NY 10027 \\
  \texttt{ss5593@columbia.edu} \\
  % examples of more authors
  \And
  Zeyu Yang \\
  zy2327 \\
  Department of Statistics\\
  Columbia University\\
  New York, NY 10027 \\
  \texttt{zy2327@columbia.edu}
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We implement DCGAN on MNIST and SVHN dataset.
  The generated samples for both datasets are great although it takes quite some time to train the model.
  
  To help the model converge more stable, we implement WGAN as well. The images it generates do not have much difference compared to the ones generated by GAN.
\end{abstract}

\section{Introduction}

Generative Adversarial Nets (GAN), first introduced by Ian Goodfellow in 2014[1], is a model that can be used to generate new images. 
It has been a hot topic since then.

The core idea of GAN is to create a two-player game:
build a generator that creates fake images while the discriminator tells whether the image is true or fake.

Through the training process, the samples generated by the generator is more similar to the true images that the discriminator can hardly tell the difference. And we want to make sure that the ability of the discriminator increases so that it could tell the difference.

The optimum case is that the generate can fully recover the distribution of the data and even the best discriminator cannot tell whether the image is fake or not i.e. the output of discriminator is $1/2$. 

In the following sections, we build a DCGAN model and train it with MNIST and SVHN datasets. We also give WGAN a try.

\section{Implementation on MNIST}

MNIST dataset is a dataset of handwritten images, including 60,000 training images and 10,000 testing images.
All digits have been size-normalized and centered in a $28\times 28$ fixed-size image[2].

The dataset has been generally used to conduct pattern recognition. 
In this project, however, we will use the training image to train our GAN model and generate similar images.

\subsection{Architecture}

We use Deep Convoltional GAN (DCGAN)[4] as our structure.
It has plenty convolutional layers and does not have max pooling layer.
We mainly use transpose convolutional layer to perform upsampling in our model.

\subsubsection{Generator}

The input of the first dense layer is a $100\times 1$ random noise vector. 
We project it to $12,544$ neurons, apply batch normalization and activation function Leaky Relu, and reshape it to $7\times 7\times 256$.

The second Convoltional Transpose layer shrinks the size of filter and outputs $7\times 7\times 128$.

The third and fourth Convolutional layers have stride 2 and padding \textit{same}. They alter the first two dimensions of the input and make the input from $7\times 7$ to $14\times 14$ to $28\times 28$.
The filter makes the third dimension from 128 to 64 to 1.

Thus the final output is a $28\times 28\times 1$ black and white image.

\subsubsection{Discriminator}

The discriminator takes in a $28\times 28\times 1$ array. After three convolutional layers and one dense layer, the output will be a scalar.
This scalar will be applied to a logistics function in future steps to be a probability that lies between 0 and 1. It is the probability of the input being a true image or a fake image.


\begin{multicols*}{2}
\textbf{Generator}
 \begin{itemize}
  \item First layer: Dense
    \begin{itemize}
      \item Input: $100 \times 1$ vector
      \item Units: $12,544$
      \item Batch normalization
      \item Activation: leaky relu
      \item Reshape $12,544$ to $7\times 7\times 256$
    \end{itemize}
  \item Second layer: Conv2DTranspose
    \begin{itemize}
      \item Input $7\times 7\times 256$
      \item Filter: 128
      \item Kernel size: 5
      \item Stride: 1
      \item Padding: same
      \item Batch normalization
      \item Activation: leaky relu
  \end{itemize} 
  \item Third layer: Conv2DTranspose
    \begin{itemize}
      \item Input $7\times 7\times 128$
      \item Filter: 64
      \item Kernel size: 5
      \item Stride: 2
      \item Padding: same
      \item Batch normalization
      \item Activation: leaky relu
    \end{itemize}   
  \item Fourth layer: Conv2DTranspose
    \begin{itemize}
      \item Input $14\times 14\times 64$
      \item Filter: 1
      \item Kernel size: 5
      \item Stride: 2
      \item Padding: same
      \item Activation: tanh
      \item Output $28\times 28\times 1$
    \end{itemize}   
\end{itemize}

\vfill\null
\columnbreak

\textbf{Discriminator}
\begin{itemize}
  \item First layer: Conv2D
    \begin{itemize}
      \item Input: $28 \times 28\times 1$ array
      \item Filter: 64
      \item Kernel size: 5
      \item Stride: 2
      \item Padding: same
      \item Activation: leaky relu
      \item Dropout: 0.3
  \end{itemize} 
  \item Third layer: Conv2D
    \begin{itemize}
      \item Filter: 128
      \item Kernel size: 5
      \item Stride: 2
      \item Padding: same
      \item Activation: leaky relu
      \item Dropout: 0.3
    \end{itemize}   
  \item Fourth layer: Conv2D
    \begin{itemize}
      \item Filter: 256
      \item Kernel size: 5
      \item Stride: 2
      \item Padding: same
      \item Activation: leaky relu
      \item Dropout: 0.3
    \end{itemize} 
  \item Fifth layer: Flatten
  \item Sixth layer: Dense with output $1\times 1$
\end{itemize}
\end{multicols*}

\subsubsection{Hyperparameters}

\begin{itemize}
  \item Epoch: 100
  \item Batch: 256
  \item Learning rate
    \begin{itemize}
      \item Generator: 1e-3
      \item Discriminator: 1e-4
    \end{itemize}
\end{itemize}

We have experimented on different bacthes and epoches and we believe a batch of 256 samples and 100 epoches already produced satisfying result. 
In fact, the quality of the image does not seem to have significant improvement after 50 epoches.

The reason why we choose the learning rate for the discriminator 10 times smaller than the learning rate for generator is 
because we noticed the loss of discriminator is much smaller than the loss of generator. 
To avoid the gradient jumping back and forth around the optimum spot, we lower the learning rate for discriminator.
The training result shows that the loss for both of them are at the same magnitude after the modification.

\subsection{Result}

\subsubsection{Tensorboard}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/tensorboard-GAN-MNIST.png}
  \caption{Tensorboard GAN MNSIT}
  \label{fig:TB_GAN_MNSIT}
\end{figure}

As we can observe from Figure \ref{fig:TB_GAN_MNSIT}, training loss of both generator and discriminator is fluctuating. 
And generally when the loss of generator decreases, the loss of discriminator increases. This is the representation of the two player game i.e. both players want to minimize their own loss.

Although the loss remains around the same level and does not continue decrease, the quality of the images are in fact improving as we can see in the following section.

\subsubsection{Generated samples}

\begin{figure}[!htb]
  \centering
  \subfloat[MNIST data]{
  \includegraphics[width=0.2\textwidth]{imgs/mnist_data.png}}
  \subfloat[Epoch 10/100]{
  \includegraphics[width=0.2\textwidth]{imgs/I1.png}}
  \subfloat[Epoch 30/100]{
  \includegraphics[width=0.2\textwidth]{imgs/I2.png}}
  \subfloat[Epoch 100/100]{
  \includegraphics[width=0.2\textwidth]{imgs/I4_after-296-batches.png}}
  \caption{Comparison of MNIST data and generated samples from DCGAN}
  \label{fig_DCGAN_MNIST}
\end{figure}

From Figure \ref{fig_DCGAN_MNIST}, 
we can see the generated samples has a good approximation after 30 epoches.
The samples from the 10th are quite blury and they cannot have a good representation of number 2, 5, 8, etc.
The latter samples are clearer and the edges of the numbers are smoother.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.5\textwidth]{imgs/MNIST_digits.png}
  \caption{MNSIT GAN generated digits}
  \label{fig_MNSIT_digits}
\end{figure}

From the generated images\ref{fig_MNSIT_digits}, we pick out some representive samples for each digit. We find out that 

a) some digits such as 5 and 8 have some redundant white marks around the edges. This is a huge difference from the original MNIST dataset whose images are clear and sharp. If discriminator could capture this difference, it might have a better judgement between true images and fake ones.

b) some digits such as 4, 1, 8 is easier to be generated while 2 and 3 is harder to be generated. This conclusion is drawn from the fact that we go through more images to find a good sample of 2 and 3. But it might be because the some digits take up more feaible space of the input noise. We cannot know for sure.

c) Digit 9 tends to transform to other digits. For example, the 1st 9 looks a little like 7 and the 4th 9 looks a little like 8.

Although not all samples are of high quality, there are a proportion of the images that have the quality of the samples presented in paper[1] 2(a).

\section{Implementation on SVHN}

SVHN dataset is a dataset similar to MNIST, containing real-world digit images that obtained from house numbers in Google Street View images.
The images have been cropped into $32\times 32$ fixed-size array with three channels[3].

In this project, we will again use the training image to train our GAN model and generate similar images.

The challenge of this implementation is that the SVHN images are much more complicated than the MNIST images. 
There are more colors such as red and blue instead of black and white. The fonts are varied. And a proportion of the images are blury.

\subsection{Architecture}

\subsubsection{Generator}

The generator is slightly modifed from the generator for MNIST. 
The output of the first dense layer is changed to $8\times 8\times 256$ 
and the filter of the fourth layer (Conv2DTranspose) has changed from 1 to 3
so that the output of the whole generator would be a $32\times 32\times 3$ image.

\subsubsection{Discriminator}

Discriminator is exactly the same as the previous one except the input is $32\times 32\times 3$ instead of $28\times 28\times 1$.

\subsubsection{Hyperparameters}

\begin{itemize}
  \item Epoch: 200
  \item Batch: 256
  \item Learning rate
    \begin{itemize}
      \item Generator: 1e-3
      \item Discriminator: 1e-4
    \end{itemize}
\end{itemize}

Since the SVHN dataset is more complicated, we train the model with 200 epoches.

\subsection{Result}

\subsubsection{Tensorboard}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/tensorboard-GAN-SVHN.png}
  \caption{Tensorboard GAN SVHN}
  \label{fig_TB_GAN_SVHN}
\end{figure}

The training loss is fluctuating as well. We can see from Figure \ref{fig_TB_GAN_SVHN} that the training loss of the discriminator keeps increasing after 150 steps (50 epoches). This implies that the generator can generate the samples so well that the discriminator can hardly distinguish the true and fake image.

\subsubsection{Generated samples}

\begin{figure}[!htb]
  \centering
  \subfloat[SVHN data]{
  \includegraphics[width=0.2\textwidth]{imgs/svhn_data.png}}
  \subfloat[Epoch 30/200]{
  \includegraphics[width=0.2\textwidth]{imgs/s1_after-103-batches.png}}
  \subfloat[Epoch 150/200]{
  \includegraphics[width=0.2\textwidth]{imgs/s2_after-514-batches.png}}
  \subfloat[Epoch 200/200]{
  \includegraphics[width=0.2\textwidth]{imgs/s3_after-590-batches.png}}
  \caption{Comparison of SVHN data and generated samples from DCGAN}
  \label{fig_DCGAN_SVHN}
\end{figure}

The quality of the generated samples are not as good as the MNIST samples since the SVHN dataset is more complicated.
This complexity makes the model harder to converge i.e. uses more epoches to get good generated samples.

Although complicated as the dataset is, the generated samples demonstrate some qualities such as the variety of the samples. There are different combinations of gree, white, and red background with white, blue, and red fonts.
There are also a proportion of one-digit images and a proportion of the two-digits images.

\section{WGAN}

Wasserstein GAN(WGAN) is a modified GAN introduced by Arjovsky, Martin, et al.(2017)[5]. 
It proposed Wasserstein distance that has a better property than Jensen-Shannon divergence.

Down to the implementation, the main difference is the loss function. We modified the loss function from \textit{BinaryCrossentropy} to a customized loss function.

The generator and discriminator are exactly the same as the GAN.

The hyperparameters are the same except this time we only train 50 epoches for each model.

\subsection{WGAN on MNIST}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/tensorboard-WGAN-MNIST.png}
  \caption{Tensorboard WGAN MNSIT}
  \label{fig_TB_WGAN_MNSIT}
\end{figure}

\begin{figure}[!htb]
  \centering
  \subfloat[MNIST data]{
  \includegraphics[width=0.2\textwidth]{imgs/mnist_data.png}}
  \subfloat[Epoch 10/50]{
  \includegraphics[width=0.2\textwidth]{imgs/I5_after-31-batches.png}}
  \subfloat[Epoch 30/50]{
  \includegraphics[width=0.2\textwidth]{imgs/I6_after-101-batches.png}}
  \subfloat[Epoch 50/50]{
  \includegraphics[width=0.2\textwidth]{imgs/I7_after-149-batches.png}}
  \caption{Comparison of MNIST data and generated samples from WGAN}
\end{figure}

The tensorboard\ref{fig_TB_WGAN_MNSIT} shows that the training process is quite stable. The training loss of the generator rapidly decreases to smaller than $0.$ within 10 epoches. After that, it remains the low level.

As of the samples, there is no significant difference between the two models. Both qualities are satisfying.

\subsection{WGAN on SVHN}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/tensorboard-WGAN-SVHN.png}
  \caption{Tensorboard WGAN SVHN}
  \label{fig_TB_WGAN_SVHN}
\end{figure}

\begin{figure}[!htb]
  \centering
  \subfloat[SVHN data]{
  \includegraphics[width=0.2\textwidth]{imgs/svhn_data.png}}
  \subfloat[Epoch 10/50]{
  \includegraphics[width=0.2\textwidth]{imgs/s4_after-30-batches.png}}
  \subfloat[Epoch 30/50]{
  \includegraphics[width=0.2\textwidth]{imgs/s5_after-102-batches.png}}
  \subfloat[Epoch 50/50]{
  \includegraphics[width=0.2\textwidth]{imgs/s6_after-142-batches.png}}
  \caption{Comparison of SVHN data and generated samples from WGAN}
\end{figure}

\section{Summary}

In short, we have studied the generative adversarial nets, especially the DCGAN and WGAN. We implemented these two models to the MNIST and WGAN dataset respectively and obtained satisfying samples.

Throughout the project, we have only tuned a few hyperparameters such as learning rate and batches. There are other hyperparameters can be tuned. In the next step, we would tune the dimension of the noise, the structure of the model, etc.

Since plenty new models have been published other than WGAN, in the future, we can try other models such as triple-GAN[6], Self-attention neural networkn[7], LOGAN[8] etc.

\section*{References}

[1] Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014.

[2] LeCun, Yann, et al. “THE MNIST DATABASE.” MNIST Handwritten Digit Database, http://yann.lecun.com/exdb/mnist/.

[3] Netzer, Yuval, et al. "Reading digits in natural images with unsupervised feature learning." (2011).

[4] Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." arXiv preprint arXiv:1511.06434 (2015).

[5] Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein gan." arXiv preprint arXiv:1701.07875 (2017).

[6] Chongxuan, L. I., et al. "Triple generative adversarial nets." Advances in neural information processing systems. 2017.

[7] Zhang, Han, et al. "Self-attention generative adversarial networks." arXiv preprint arXiv:1805.08318 (2018).

[8] Wu, Yan, et al. "LOGAN: Latent Optimisation for Generative Adversarial Networks." arXiv preprint arXiv:1912.00953 (2019).

\end{document}
