{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GANs-CoLab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# Deep Convolutional Generative Adversarial Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGM8xKZrDgHD"
      },
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b0uoZZ7fDgHE"
      },
      "source": [
        "### Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rsfJO0qoEuNE",
        "colab": {}
      },
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow-gpu\n",
        "# !pip install --upgrade grpcio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp_iux5-Rig6",
        "colab_type": "code",
        "outputId": "98287f83-1792-42c1-ebe6-ab3271d02986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HWGkAtMXDgHE",
        "outputId": "b3528b20-47c4-4b29-c3f2-09c51021ce0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "import imageio\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"%d GPUs are available for tensorflow %s in current environment.\" % \n",
        "      (len(tf.config.experimental.list_physical_devices('GPU')), tf.__version__))\n",
        "# %load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 GPUs are available for tensorflow 2.0.0 in current environment.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jsEAA1m5DgHG"
      },
      "source": [
        "### Set the Paths of the Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TouXUserDgHH",
        "colab": {}
      },
      "source": [
        "# get the dictionary for the project\n",
        "pwd = '/drive/My Drive/Colab Notebooks/2019Fall-AML-FP-GAN'\n",
        "# set and create the path for log file for tesnorboard\n",
        "log_dir = os.path.join(pwd, 'outputs', 'logs')\n",
        "os.makedirs(log_dir, exist_ok = True)\n",
        "# set and create the path for saving the images\n",
        "image_dir = os.path.join(pwd, 'outputs', 'images')\n",
        "os.makedirs(image_dir, exist_ok = True)\n",
        "# set and create the path for saving the weights of the model\n",
        "checkpoint_dir = os.path.join(pwd, 'outputs', 'checkpoints')\n",
        "os.makedirs(checkpoint_dir, exist_ok = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZC_fHe_DgHJ",
        "outputId": "926d58a1-5985-4b5d-873c-55412aefd3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# config the version of training, structure of model and usage of dataset\n",
        "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "structure = 'GAN'\n",
        "dataset = 'MNIST'\n",
        "config = \"%s-%s-%s\" % (structure, dataset, stamp)\n",
        "config"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GAN-MNIST-20191127-013034'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4fYMGxGhrna",
        "colab": {}
      },
      "source": [
        "# load mnist data\n",
        "(mnist_train, _), (mnist_val, _) = tf.keras.datasets.mnist.load_data()\n",
        "# load svhn data\n",
        "svhn_train = tfds.load(\"svhn_cropped\", with_info=False, as_supervised=True, split=tfds.Split.TRAIN)\n",
        "svhn_val = tfds.load(\"svhn_cropped\", with_info=False, as_supervised=True, split=tfds.Split.TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "11DxF_d8DgHM",
        "colab": {}
      },
      "source": [
        "# normalize data\n",
        "def mnist_preprocess(x):        \n",
        "    x = 2*tf.cast(x, dtype=tf.float32)/255 - 1\n",
        "    # expand the 3d tensor to 4d\n",
        "    x = tf.expand_dims(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "def svhn_preprocess(x, y):        \n",
        "    x = 2*tf.cast(x, dtype=tf.float32)/255 - 1      \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-yKCCQOoJ7cn",
        "outputId": "5eb99c71-f5a7-4ae2-f626-81f6c29ac044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# preprocess the MNIST dataset\n",
        "mnist_train_db = tf.data.Dataset.from_tensor_slices(mnist_train).map(mnist_preprocess).shuffle(60000)\n",
        "mnist_val_db = tf.data.Dataset.from_tensor_slices(mnist_val).map(mnist_preprocess)\n",
        "# get one batch and check the dimension of this batch\n",
        "mnist_samples = next(iter(mnist_val_db.batch(32)))\n",
        "print(\"shape of one batch for MNIST images is: \" + str(mnist_samples.shape))\n",
        "# preprocess the SVHN dataset\n",
        "svhn_train_db = svhn_train.map(svhn_preprocess).shuffle(60000)\n",
        "svhn_val_db = svhn_val.map(svhn_preprocess)\n",
        "svhn_samples = next(iter(svhn_val_db.batch(32)))\n",
        "print(\"shape of one batch for SVHN images is: \" + str(svhn_samples.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of one batch for MNIST images is: (32, 28, 28, 1)\n",
            "shape of one batch for SVHN images is: (32, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Step 2: Define the Generator and Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w0CnPpMfDgHS",
        "colab": {}
      },
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, size, channel):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc0 = layers.Dense(size*size*256, use_bias=False)\n",
        "        self.bn0 = layers.BatchNormalization()\n",
        "        \n",
        "        self.conv1 = layers.Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        \n",
        "        self.conv2 = layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        \n",
        "        self.conv3 = layers.Conv2DTranspose(filters=channel, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
        "\n",
        "        self.size = size\n",
        "         \n",
        "    def call(self, inputs, training=None):\n",
        "        x = self.fc0(inputs)   \n",
        "        x = tf.reshape(x, [-1, self.size, self.size, 256]) \n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        \n",
        "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
        "        \n",
        "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
        "        \n",
        "        image = tf.math.tanh(self.conv3(x))\n",
        "        \n",
        "        return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZK5odTrNDgHT",
        "outputId": "5ab33b56-ff8e-465e-9063-ecd0089ae79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if dataset == 'MNIST':\n",
        "    generator = Generator(size = 7, channel = 1)\n",
        "elif dataset == 'SVHN':\n",
        "    generator = Generator(size = 8, channel = 3)\n",
        "seed = tf.random.normal([32, 100])\n",
        "image_fake = generator(seed, training=False)\n",
        "print(\"The shape of the output of the generator is: \" + str(image_fake.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the output of the generator is: (32, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xHakGKzeDgHU",
        "outputId": "db594dba-1265-4b48-fdd3-c24086fc011d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "if dataset == 'MNIST':\n",
        "    plt.imshow(image_fake[0, :, :, 0], cmap='gray')\n",
        "    plt.show()\n",
        "else:\n",
        "    plt.imshow(image_fake[0, :, :, :])\n",
        "    plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWx887DtDgHW",
        "colab": {}
      },
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same')\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        \n",
        "        self.conv2 = layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "        \n",
        "        self.conv3 = layers.Conv2D(filters=256, kernel_size=5, strides=2, padding='same')\n",
        "        self.bn3 = layers.BatchNormalization()\n",
        "        self.dropout3 = layers.Dropout(0.3)\n",
        "        \n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc = layers.Dense(1)\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        x = tf.nn.leaky_relu(self.dropout1(self.conv1(inputs)))\n",
        "        x = self.dropout2(tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training)))\n",
        "        x = self.dropout3(tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training)))\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        logits = self.fc(x)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wExWVnv0DgHY",
        "outputId": "fea292e1-3e3d-4a1d-e174-d23d66cb23fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "discriminator = Discriminator()\n",
        "logits = discriminator(image_fake, training=False)\n",
        "print(\"The shape of logits given by discriminator is: \" + str(logits.shape))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of logits given by discriminator is: (32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Step 3: Define some Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yPhPSHepDgHa",
        "colab": {}
      },
      "source": [
        "loss_cal = tf.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpl0eJ4UP9Ix",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Penalty Calculator for WGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjj9Ay-kP9Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_penalty_cal(discriminator, image_fake, image_true):\n",
        "    \n",
        "    # [b, 1, 1, 1] -> [b, h, w, c]\n",
        "    batch = image_true.shape[0]\n",
        "    alpha = tf.random.uniform([batch, 1, 1, 1])\n",
        "    alpha = tf.broadcast_to(alpha, image_true.shape)\n",
        "    \n",
        "    interplate = alpha * image_true + (1-alpha)*image_fake\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interplate)\n",
        "        interplate_logits = discriminator(interplate)\n",
        "    grads = tape.gradient(interplate_logits, interplate)\n",
        "    \n",
        "    # [b, h, w, c] -> [b, h*w*c] -> [b]\n",
        "    grads = tf.reshape(grads, [batch, -1])\n",
        "    norm = tf.norm(grads, axis=1)\n",
        "    \n",
        "    grads_penalty = tf.reduce_mean(tf.math.squared_difference(norm, 1))\n",
        "    \n",
        "    return grads_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator Loss Calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wkMNfBWlT-PV",
        "colab": {}
      },
      "source": [
        "def discriminator_loss_cal(generator, discriminator, seed, image_true, training, LAMBDA=1.):\n",
        "    \n",
        "    image_fake = generator(seed, training)\n",
        "    \n",
        "    # treat real image as real\n",
        "    logits_real = discriminator(image_true, training)\n",
        "    loss_real = tf.reduce_mean(loss_cal(tf.ones_like(logits_real), logits_real))\n",
        "    # treat fake image as fake\n",
        "    logits_fake = discriminator(image_fake, training)\n",
        "    loss_fake = tf.reduce_mean(loss_cal(tf.zeros_like(logits_fake), logits_fake)) \n",
        "    \n",
        "    loss = loss_fake + loss_real\n",
        "    if structure == 'GAN':\n",
        "        return loss\n",
        "    elif structure == 'WGAN':\n",
        "        gradient_penalty = gradient_penalty_cal(discriminator, image_true, image_fake)\n",
        "        loss += LAMBDA * gradient_penalty\n",
        "    \n",
        "    return (loss, gradient_penalty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "### Generator Loss Calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O54tCwCzDgHe",
        "colab": {}
      },
      "source": [
        "def generator_loss_cal(generator, discriminator, seed, training):\n",
        "    \n",
        "    image_fake = generator(seed, training)\n",
        "    logits_fake = discriminator(image_fake, training)\n",
        "    \n",
        "    loss = tf.reduce_mean(loss_cal(tf.ones_like(logits_fake), logits_fake))     \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H9S9hxTnDgHf"
      },
      "source": [
        "### Image Saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VSmdsjHODgHg",
        "colab": {}
      },
      "source": [
        "def save_image(images, height, width, image_path, index):\n",
        "\n",
        "    images = ((images + 1.0) * 127.5).astype(np.uint8)\n",
        "    # final picture which save as PNG\n",
        "    picture = np.array([])\n",
        "    \n",
        "    row = np.array([])\n",
        "    for i in range(height*width):\n",
        "        # concat image into a row\n",
        "        if row.size == 0:\n",
        "            row = images[i]\n",
        "        else:\n",
        "            row = np.concatenate((row, images[i]), axis=1)\n",
        "\n",
        "        # concat image row to picture\n",
        "        if (i+1) % width == 0:\n",
        "            if picture.size == 0:\n",
        "                picture = row\n",
        "            else:\n",
        "                picture = np.concatenate((picture, row), axis=0)\n",
        "            # reset single row\n",
        "            row = np.array([])\n",
        "    \n",
        "    picture_board = tf.expand_dims(tf.cast(picture, dtype=tf.uint8), 0)\n",
        "    with summary_writer.as_default(): \n",
        "        tf.summary.image('train-fake-images:', picture_board, step=index)\n",
        "    \n",
        "    if 'MNIST' in image_path:\n",
        "        picture = np.squeeze(picture, axis=2)\n",
        "        Image.fromarray(picture, 'P').save(image_path)\n",
        "    elif 'SVHN' in image_path:\n",
        "        Image.fromarray(picture, 'RGB').save(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Step 4: Define the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n2MVkQZmDgHh",
        "colab": {}
      },
      "source": [
        "# number of epochs\n",
        "EPOCH = 100\n",
        "BATCH = 256\n",
        "# step size, i.e. number of batches per validation\n",
        "STEP = 100\n",
        "SAVE = 50\n",
        "\n",
        "SEED = 100\n",
        "LAMBDA = 10.\n",
        "LEARNING_RATE_D = 1e-4\n",
        "LEARNING_RATE_G = 1e-3\n",
        "\n",
        "RESTORE = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aA1gbMUGDgHl",
        "colab": {}
      },
      "source": [
        "if dataset == 'MNIST':\n",
        "    train_db = mnist_train_db.batch(BATCH, drop_remainder=True)\n",
        "    val_db = mnist_val_db.batch(BATCH, drop_remainder=True)\n",
        "elif dataset == 'SVHN':\n",
        "    train_db = svhn_train_db.batch(BATCH, drop_remainder=True)\n",
        "    val_db = svhn_val_db.batch(BATCH, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE_G)\n",
        "discriminator_optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CA1w-7s2POEy",
        "colab": {}
      },
      "source": [
        "# save the training process\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, config, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# the log file created for tensorborad for this time of training\n",
        "cur_log_dir = os.path.join(log_dir, config)\n",
        "summary_writer = tf.summary.create_file_writer(cur_log_dir) \n",
        "\n",
        "# the file for saving images for this time of training\n",
        "cur_image_dir = os.path.join(image_dir, config)\n",
        "os.makedirs(cur_image_dir, exist_ok = True)\n",
        "\n",
        "# if you want to see the visualization during the traing, run this line\n",
        "# %tensorboard --logdir=log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cBPZljjBDgHo",
        "outputId": "243e86ef-8426-462c-db4b-18d4f15124c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# restore the pretrained model\n",
        "if RESTORE:\n",
        "    restore_checkpoint_dir = os.path.join(checkpoint_dir, restore)\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(restore_checkpoint_dir))\n",
        "    \n",
        "# number of batches have trained\n",
        "index = -1\n",
        "for epoch in range(1, EPOCH+1):\n",
        "    \n",
        "    for step, image_true in enumerate(train_db):\n",
        "        \n",
        "        seed = tf.random.uniform([BATCH, SEED], minval=-1, maxval=1)\n",
        "        # train Discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            if structure == 'GAN':\n",
        "                discriminator_loss = discriminator_loss_cal(generator, discriminator, seed, image_true, training=True)\n",
        "            elif structure == 'WGAN':\n",
        "                discriminator_loss, gradient_penalty = discriminator_loss_cal(generator, discriminator, seed, image_true, training=True)\n",
        "        grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
        "        discriminator_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "        \n",
        "        # train Generator\n",
        "        with tf.GradientTape() as tape:\n",
        "            generator_loss = generator_loss_cal(generator, discriminator, seed, training=True)\n",
        "        grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
        "        generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))     \n",
        "        \n",
        "        if step % STEP == 0:\n",
        "            index += 1\n",
        "            print(\"At epoch: %s/%d, after %s batches have been trained: discriminator loss: %.5f, generator loss: %.5f\" % \n",
        "                  (format(epoch, '2'), EPOCH, format(step,'2'), float(discriminator_loss), float(generator_loss)))\n",
        "            \n",
        "            with summary_writer.as_default(): \n",
        "                tf.summary.scalar('train-generator-loss', float(generator_loss), step=index) \n",
        "                tf.summary.scalar('train-discriminator-loss', float(discriminator_loss), step=index) \n",
        "                \n",
        "            fake_image = generator(seed, training=False)\n",
        "            image_path = os.path.join(cur_image_dir, 'after-%d-batches.png' % index)\n",
        "            save_image(fake_image.numpy(), 5, 5, image_path, index)\n",
        "    \n",
        "    if epoch % SAVE == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At epoch:  1/100, after  0 batches have been trained: discriminator loss: 2.00888, generator loss: 0.89080\n",
            "At epoch:  1/100, after 100 batches have been trained: discriminator loss: 0.99043, generator loss: 2.89097\n",
            "At epoch:  1/100, after 200 batches have been trained: discriminator loss: 1.95675, generator loss: 2.43001\n",
            "At epoch:  2/100, after  0 batches have been trained: discriminator loss: 0.62624, generator loss: 2.40418\n",
            "At epoch:  2/100, after 100 batches have been trained: discriminator loss: 0.78334, generator loss: 2.46894\n",
            "At epoch:  2/100, after 200 batches have been trained: discriminator loss: 1.06406, generator loss: 1.72251\n",
            "At epoch:  3/100, after  0 batches have been trained: discriminator loss: 1.57437, generator loss: 1.47595\n",
            "At epoch:  3/100, after 100 batches have been trained: discriminator loss: 0.82374, generator loss: 2.20773\n",
            "At epoch:  3/100, after 200 batches have been trained: discriminator loss: 1.33975, generator loss: 1.81894\n",
            "At epoch:  4/100, after  0 batches have been trained: discriminator loss: 0.48998, generator loss: 2.83657\n",
            "At epoch:  4/100, after 100 batches have been trained: discriminator loss: 0.96354, generator loss: 2.00222\n",
            "At epoch:  4/100, after 200 batches have been trained: discriminator loss: 0.89445, generator loss: 1.97534\n",
            "At epoch:  5/100, after  0 batches have been trained: discriminator loss: 0.78380, generator loss: 2.24916\n",
            "At epoch:  5/100, after 100 batches have been trained: discriminator loss: 0.69239, generator loss: 2.29396\n",
            "At epoch:  5/100, after 200 batches have been trained: discriminator loss: 1.21864, generator loss: 1.53906\n",
            "At epoch:  6/100, after  0 batches have been trained: discriminator loss: 0.80708, generator loss: 2.27877\n",
            "At epoch:  6/100, after 100 batches have been trained: discriminator loss: 0.50436, generator loss: 2.57069\n",
            "At epoch:  6/100, after 200 batches have been trained: discriminator loss: 1.02226, generator loss: 1.68371\n",
            "At epoch:  7/100, after  0 batches have been trained: discriminator loss: 0.78159, generator loss: 1.89113\n",
            "At epoch:  7/100, after 100 batches have been trained: discriminator loss: 0.72608, generator loss: 2.21560\n",
            "At epoch:  7/100, after 200 batches have been trained: discriminator loss: 1.84558, generator loss: 1.33686\n",
            "At epoch:  8/100, after  0 batches have been trained: discriminator loss: 0.60750, generator loss: 2.41891\n",
            "At epoch:  8/100, after 100 batches have been trained: discriminator loss: 0.79302, generator loss: 2.19336\n",
            "At epoch:  8/100, after 200 batches have been trained: discriminator loss: 0.46843, generator loss: 3.21930\n",
            "At epoch:  9/100, after  0 batches have been trained: discriminator loss: 1.36252, generator loss: 1.12941\n",
            "At epoch:  9/100, after 100 batches have been trained: discriminator loss: 0.95251, generator loss: 1.88370\n",
            "At epoch:  9/100, after 200 batches have been trained: discriminator loss: 0.95259, generator loss: 1.83619\n",
            "At epoch: 10/100, after  0 batches have been trained: discriminator loss: 0.99081, generator loss: 1.57074\n",
            "At epoch: 10/100, after 100 batches have been trained: discriminator loss: 0.72108, generator loss: 1.97502\n",
            "At epoch: 10/100, after 200 batches have been trained: discriminator loss: 0.97333, generator loss: 1.79266\n",
            "At epoch: 11/100, after  0 batches have been trained: discriminator loss: 0.76555, generator loss: 1.92186\n",
            "At epoch: 11/100, after 100 batches have been trained: discriminator loss: 1.41305, generator loss: 1.33865\n",
            "At epoch: 11/100, after 200 batches have been trained: discriminator loss: 0.81676, generator loss: 1.99271\n",
            "At epoch: 12/100, after  0 batches have been trained: discriminator loss: 0.79671, generator loss: 1.67494\n",
            "At epoch: 12/100, after 100 batches have been trained: discriminator loss: 0.70999, generator loss: 1.71149\n",
            "At epoch: 12/100, after 200 batches have been trained: discriminator loss: 0.83860, generator loss: 1.94992\n",
            "At epoch: 13/100, after  0 batches have been trained: discriminator loss: 1.20215, generator loss: 1.68169\n",
            "At epoch: 13/100, after 100 batches have been trained: discriminator loss: 0.63130, generator loss: 2.44810\n",
            "At epoch: 13/100, after 200 batches have been trained: discriminator loss: 0.34765, generator loss: 2.83863\n",
            "At epoch: 14/100, after  0 batches have been trained: discriminator loss: 0.78723, generator loss: 1.81226\n",
            "At epoch: 14/100, after 100 batches have been trained: discriminator loss: 0.75410, generator loss: 1.80072\n",
            "At epoch: 14/100, after 200 batches have been trained: discriminator loss: 0.69028, generator loss: 1.85265\n",
            "At epoch: 15/100, after  0 batches have been trained: discriminator loss: 0.80183, generator loss: 1.98876\n",
            "At epoch: 15/100, after 100 batches have been trained: discriminator loss: 0.52696, generator loss: 2.55424\n",
            "At epoch: 15/100, after 200 batches have been trained: discriminator loss: 0.85865, generator loss: 2.13322\n",
            "At epoch: 16/100, after  0 batches have been trained: discriminator loss: 0.57442, generator loss: 2.51699\n",
            "At epoch: 16/100, after 100 batches have been trained: discriminator loss: 0.62885, generator loss: 2.29064\n",
            "At epoch: 16/100, after 200 batches have been trained: discriminator loss: 0.59997, generator loss: 2.42812\n",
            "At epoch: 17/100, after  0 batches have been trained: discriminator loss: 0.65789, generator loss: 2.35345\n",
            "At epoch: 17/100, after 100 batches have been trained: discriminator loss: 0.72090, generator loss: 2.08793\n",
            "At epoch: 17/100, after 200 batches have been trained: discriminator loss: 0.87481, generator loss: 1.81410\n",
            "At epoch: 18/100, after  0 batches have been trained: discriminator loss: 0.51597, generator loss: 2.57752\n",
            "At epoch: 18/100, after 100 batches have been trained: discriminator loss: 0.67526, generator loss: 2.31477\n",
            "At epoch: 18/100, after 200 batches have been trained: discriminator loss: 0.75035, generator loss: 1.85355\n",
            "At epoch: 19/100, after  0 batches have been trained: discriminator loss: 0.78096, generator loss: 2.13310\n",
            "At epoch: 19/100, after 100 batches have been trained: discriminator loss: 0.99169, generator loss: 1.77691\n",
            "At epoch: 19/100, after 200 batches have been trained: discriminator loss: 0.67329, generator loss: 2.08339\n",
            "At epoch: 20/100, after  0 batches have been trained: discriminator loss: 0.59896, generator loss: 2.50330\n",
            "At epoch: 20/100, after 100 batches have been trained: discriminator loss: 0.57556, generator loss: 2.53323\n",
            "At epoch: 20/100, after 200 batches have been trained: discriminator loss: 0.69645, generator loss: 2.46992\n",
            "At epoch: 21/100, after  0 batches have been trained: discriminator loss: 0.51860, generator loss: 2.28749\n",
            "At epoch: 21/100, after 100 batches have been trained: discriminator loss: 0.75456, generator loss: 2.11377\n",
            "At epoch: 21/100, after 200 batches have been trained: discriminator loss: 0.53993, generator loss: 2.24559\n",
            "At epoch: 22/100, after  0 batches have been trained: discriminator loss: 0.81707, generator loss: 1.93085\n",
            "At epoch: 22/100, after 100 batches have been trained: discriminator loss: 0.51845, generator loss: 2.65431\n",
            "At epoch: 22/100, after 200 batches have been trained: discriminator loss: 0.66424, generator loss: 1.83086\n",
            "At epoch: 23/100, after  0 batches have been trained: discriminator loss: 0.46816, generator loss: 2.33464\n",
            "At epoch: 23/100, after 100 batches have been trained: discriminator loss: 0.69576, generator loss: 2.12660\n",
            "At epoch: 23/100, after 200 batches have been trained: discriminator loss: 0.63284, generator loss: 2.34192\n",
            "At epoch: 24/100, after  0 batches have been trained: discriminator loss: 0.50078, generator loss: 2.57806\n",
            "At epoch: 24/100, after 100 batches have been trained: discriminator loss: 0.59600, generator loss: 2.18377\n",
            "At epoch: 24/100, after 200 batches have been trained: discriminator loss: 0.44189, generator loss: 2.57551\n",
            "At epoch: 25/100, after  0 batches have been trained: discriminator loss: 0.61248, generator loss: 1.99888\n",
            "At epoch: 25/100, after 100 batches have been trained: discriminator loss: 0.33015, generator loss: 2.51149\n",
            "At epoch: 25/100, after 200 batches have been trained: discriminator loss: 0.44997, generator loss: 3.24574\n",
            "At epoch: 26/100, after  0 batches have been trained: discriminator loss: 0.44056, generator loss: 2.40312\n",
            "At epoch: 26/100, after 100 batches have been trained: discriminator loss: 0.54666, generator loss: 2.19636\n",
            "At epoch: 26/100, after 200 batches have been trained: discriminator loss: 0.43073, generator loss: 2.82249\n",
            "At epoch: 27/100, after  0 batches have been trained: discriminator loss: 0.50116, generator loss: 1.84126\n",
            "At epoch: 27/100, after 100 batches have been trained: discriminator loss: 0.36741, generator loss: 3.04308\n",
            "At epoch: 27/100, after 200 batches have been trained: discriminator loss: 0.54847, generator loss: 2.42373\n",
            "At epoch: 28/100, after  0 batches have been trained: discriminator loss: 0.81574, generator loss: 2.02067\n",
            "At epoch: 28/100, after 100 batches have been trained: discriminator loss: 0.41917, generator loss: 2.68882\n",
            "At epoch: 28/100, after 200 batches have been trained: discriminator loss: 0.46685, generator loss: 2.17153\n",
            "At epoch: 29/100, after  0 batches have been trained: discriminator loss: 0.50387, generator loss: 2.21415\n",
            "At epoch: 29/100, after 100 batches have been trained: discriminator loss: 0.60435, generator loss: 1.67586\n",
            "At epoch: 29/100, after 200 batches have been trained: discriminator loss: 0.72462, generator loss: 1.86991\n",
            "At epoch: 30/100, after  0 batches have been trained: discriminator loss: 0.51761, generator loss: 1.80120\n",
            "At epoch: 30/100, after 100 batches have been trained: discriminator loss: 0.63589, generator loss: 2.63167\n",
            "At epoch: 30/100, after 200 batches have been trained: discriminator loss: 0.47637, generator loss: 2.36310\n",
            "At epoch: 31/100, after  0 batches have been trained: discriminator loss: 0.34954, generator loss: 2.54885\n",
            "At epoch: 31/100, after 100 batches have been trained: discriminator loss: 0.71736, generator loss: 2.85894\n",
            "At epoch: 31/100, after 200 batches have been trained: discriminator loss: 0.58172, generator loss: 2.55354\n",
            "At epoch: 32/100, after  0 batches have been trained: discriminator loss: 0.63113, generator loss: 2.09036\n",
            "At epoch: 32/100, after 100 batches have been trained: discriminator loss: 0.69377, generator loss: 2.18408\n",
            "At epoch: 32/100, after 200 batches have been trained: discriminator loss: 0.55406, generator loss: 2.27812\n",
            "At epoch: 33/100, after  0 batches have been trained: discriminator loss: 0.72924, generator loss: 2.20696\n",
            "At epoch: 33/100, after 100 batches have been trained: discriminator loss: 0.91687, generator loss: 1.88587\n",
            "At epoch: 33/100, after 200 batches have been trained: discriminator loss: 0.79282, generator loss: 2.10411\n",
            "At epoch: 34/100, after  0 batches have been trained: discriminator loss: 0.61465, generator loss: 1.93268\n",
            "At epoch: 34/100, after 100 batches have been trained: discriminator loss: 0.83431, generator loss: 2.27801\n",
            "At epoch: 34/100, after 200 batches have been trained: discriminator loss: 0.72967, generator loss: 1.84521\n",
            "At epoch: 35/100, after  0 batches have been trained: discriminator loss: 0.85801, generator loss: 1.97884\n",
            "At epoch: 35/100, after 100 batches have been trained: discriminator loss: 0.72574, generator loss: 2.56448\n",
            "At epoch: 35/100, after 200 batches have been trained: discriminator loss: 0.66931, generator loss: 2.00819\n",
            "At epoch: 36/100, after  0 batches have been trained: discriminator loss: 0.73453, generator loss: 2.52968\n",
            "At epoch: 36/100, after 100 batches have been trained: discriminator loss: 0.94395, generator loss: 2.11021\n",
            "At epoch: 36/100, after 200 batches have been trained: discriminator loss: 0.58302, generator loss: 2.16412\n",
            "At epoch: 37/100, after  0 batches have been trained: discriminator loss: 0.87048, generator loss: 1.56745\n",
            "At epoch: 37/100, after 100 batches have been trained: discriminator loss: 0.60863, generator loss: 1.97909\n",
            "At epoch: 37/100, after 200 batches have been trained: discriminator loss: 0.58199, generator loss: 2.76711\n",
            "At epoch: 38/100, after  0 batches have been trained: discriminator loss: 0.82996, generator loss: 1.19785\n",
            "At epoch: 38/100, after 100 batches have been trained: discriminator loss: 0.55592, generator loss: 2.17772\n",
            "At epoch: 38/100, after 200 batches have been trained: discriminator loss: 1.01545, generator loss: 0.99675\n",
            "At epoch: 39/100, after  0 batches have been trained: discriminator loss: 0.82058, generator loss: 2.50954\n",
            "At epoch: 39/100, after 100 batches have been trained: discriminator loss: 0.69495, generator loss: 2.02480\n",
            "At epoch: 39/100, after 200 batches have been trained: discriminator loss: 0.63709, generator loss: 1.62797\n",
            "At epoch: 40/100, after  0 batches have been trained: discriminator loss: 0.86640, generator loss: 1.40655\n",
            "At epoch: 40/100, after 100 batches have been trained: discriminator loss: 0.84032, generator loss: 2.25316\n",
            "At epoch: 40/100, after 200 batches have been trained: discriminator loss: 0.58495, generator loss: 2.16517\n",
            "At epoch: 41/100, after  0 batches have been trained: discriminator loss: 0.60160, generator loss: 1.75318\n",
            "At epoch: 41/100, after 100 batches have been trained: discriminator loss: 0.84208, generator loss: 1.94062\n",
            "At epoch: 41/100, after 200 batches have been trained: discriminator loss: 0.74988, generator loss: 1.48118\n",
            "At epoch: 42/100, after  0 batches have been trained: discriminator loss: 0.83343, generator loss: 2.05159\n",
            "At epoch: 42/100, after 100 batches have been trained: discriminator loss: 0.62976, generator loss: 2.31963\n",
            "At epoch: 42/100, after 200 batches have been trained: discriminator loss: 0.69612, generator loss: 1.67659\n",
            "At epoch: 43/100, after  0 batches have been trained: discriminator loss: 0.81100, generator loss: 2.40627\n",
            "At epoch: 43/100, after 100 batches have been trained: discriminator loss: 0.78466, generator loss: 1.60404\n",
            "At epoch: 43/100, after 200 batches have been trained: discriminator loss: 0.65728, generator loss: 1.95021\n",
            "At epoch: 44/100, after  0 batches have been trained: discriminator loss: 0.79235, generator loss: 2.54498\n",
            "At epoch: 44/100, after 100 batches have been trained: discriminator loss: 0.82078, generator loss: 1.56999\n",
            "At epoch: 44/100, after 200 batches have been trained: discriminator loss: 0.96178, generator loss: 1.19638\n",
            "At epoch: 45/100, after  0 batches have been trained: discriminator loss: 1.01867, generator loss: 1.47010\n",
            "At epoch: 45/100, after 100 batches have been trained: discriminator loss: 0.79106, generator loss: 2.56244\n",
            "At epoch: 45/100, after 200 batches have been trained: discriminator loss: 0.78083, generator loss: 2.25986\n",
            "At epoch: 46/100, after  0 batches have been trained: discriminator loss: 0.75905, generator loss: 1.45266\n",
            "At epoch: 46/100, after 100 batches have been trained: discriminator loss: 0.99360, generator loss: 1.82280\n",
            "At epoch: 46/100, after 200 batches have been trained: discriminator loss: 0.84855, generator loss: 2.69391\n",
            "At epoch: 47/100, after  0 batches have been trained: discriminator loss: 0.80114, generator loss: 1.97259\n",
            "At epoch: 47/100, after 100 batches have been trained: discriminator loss: 0.61514, generator loss: 1.89686\n",
            "At epoch: 47/100, after 200 batches have been trained: discriminator loss: 0.87488, generator loss: 1.94982\n",
            "At epoch: 48/100, after  0 batches have been trained: discriminator loss: 0.57147, generator loss: 2.29572\n",
            "At epoch: 48/100, after 100 batches have been trained: discriminator loss: 0.92073, generator loss: 1.24102\n",
            "At epoch: 48/100, after 200 batches have been trained: discriminator loss: 0.88982, generator loss: 2.34105\n",
            "At epoch: 49/100, after  0 batches have been trained: discriminator loss: 1.14093, generator loss: 1.37453\n",
            "At epoch: 49/100, after 100 batches have been trained: discriminator loss: 0.68061, generator loss: 1.46338\n",
            "At epoch: 49/100, after 200 batches have been trained: discriminator loss: 0.70708, generator loss: 2.43022\n",
            "At epoch: 50/100, after  0 batches have been trained: discriminator loss: 0.84733, generator loss: 2.11189\n",
            "At epoch: 50/100, after 100 batches have been trained: discriminator loss: 1.03349, generator loss: 1.76759\n",
            "At epoch: 50/100, after 200 batches have been trained: discriminator loss: 0.89713, generator loss: 1.98821\n",
            "At epoch: 51/100, after  0 batches have been trained: discriminator loss: 0.77767, generator loss: 1.57774\n",
            "At epoch: 51/100, after 100 batches have been trained: discriminator loss: 0.51281, generator loss: 1.91994\n",
            "At epoch: 51/100, after 200 batches have been trained: discriminator loss: 0.90159, generator loss: 1.79772\n",
            "At epoch: 52/100, after  0 batches have been trained: discriminator loss: 0.93703, generator loss: 1.32741\n",
            "At epoch: 52/100, after 100 batches have been trained: discriminator loss: 0.86033, generator loss: 1.82848\n",
            "At epoch: 52/100, after 200 batches have been trained: discriminator loss: 1.40100, generator loss: 1.15844\n",
            "At epoch: 53/100, after  0 batches have been trained: discriminator loss: 0.89900, generator loss: 2.19176\n",
            "At epoch: 53/100, after 100 batches have been trained: discriminator loss: 0.97557, generator loss: 1.21965\n",
            "At epoch: 53/100, after 200 batches have been trained: discriminator loss: 0.72826, generator loss: 1.95958\n",
            "At epoch: 54/100, after  0 batches have been trained: discriminator loss: 0.91604, generator loss: 1.43856\n",
            "At epoch: 54/100, after 100 batches have been trained: discriminator loss: 0.69510, generator loss: 1.85821\n",
            "At epoch: 54/100, after 200 batches have been trained: discriminator loss: 1.00448, generator loss: 1.39131\n",
            "At epoch: 55/100, after  0 batches have been trained: discriminator loss: 0.95644, generator loss: 2.75390\n",
            "At epoch: 55/100, after 100 batches have been trained: discriminator loss: 0.99787, generator loss: 2.50051\n",
            "At epoch: 55/100, after 200 batches have been trained: discriminator loss: 0.81893, generator loss: 2.06758\n",
            "At epoch: 56/100, after  0 batches have been trained: discriminator loss: 0.69283, generator loss: 1.74120\n",
            "At epoch: 56/100, after 100 batches have been trained: discriminator loss: 0.88883, generator loss: 1.99902\n",
            "At epoch: 56/100, after 200 batches have been trained: discriminator loss: 0.98184, generator loss: 1.84638\n",
            "At epoch: 57/100, after  0 batches have been trained: discriminator loss: 0.91233, generator loss: 1.39221\n",
            "At epoch: 57/100, after 100 batches have been trained: discriminator loss: 0.89409, generator loss: 2.92475\n",
            "At epoch: 57/100, after 200 batches have been trained: discriminator loss: 0.73471, generator loss: 1.64600\n",
            "At epoch: 58/100, after  0 batches have been trained: discriminator loss: 0.76441, generator loss: 2.47849\n",
            "At epoch: 58/100, after 100 batches have been trained: discriminator loss: 0.90658, generator loss: 2.17169\n",
            "At epoch: 58/100, after 200 batches have been trained: discriminator loss: 0.93880, generator loss: 1.63773\n",
            "At epoch: 59/100, after  0 batches have been trained: discriminator loss: 0.69098, generator loss: 1.48711\n",
            "At epoch: 59/100, after 100 batches have been trained: discriminator loss: 0.67436, generator loss: 1.55288\n",
            "At epoch: 59/100, after 200 batches have been trained: discriminator loss: 0.95783, generator loss: 1.90401\n",
            "At epoch: 60/100, after  0 batches have been trained: discriminator loss: 0.89949, generator loss: 1.18629\n",
            "At epoch: 60/100, after 100 batches have been trained: discriminator loss: 0.71815, generator loss: 2.31354\n",
            "At epoch: 60/100, after 200 batches have been trained: discriminator loss: 0.93290, generator loss: 2.16678\n",
            "At epoch: 61/100, after  0 batches have been trained: discriminator loss: 0.80282, generator loss: 2.39053\n",
            "At epoch: 61/100, after 100 batches have been trained: discriminator loss: 0.60666, generator loss: 2.47738\n",
            "At epoch: 61/100, after 200 batches have been trained: discriminator loss: 0.78073, generator loss: 1.51643\n",
            "At epoch: 62/100, after  0 batches have been trained: discriminator loss: 1.00280, generator loss: 1.73183\n",
            "At epoch: 62/100, after 100 batches have been trained: discriminator loss: 0.82788, generator loss: 1.45282\n",
            "At epoch: 62/100, after 200 batches have been trained: discriminator loss: 0.72309, generator loss: 1.68385\n",
            "At epoch: 63/100, after  0 batches have been trained: discriminator loss: 1.35017, generator loss: 0.54027\n",
            "At epoch: 63/100, after 100 batches have been trained: discriminator loss: 1.12176, generator loss: 1.01702\n",
            "At epoch: 63/100, after 200 batches have been trained: discriminator loss: 0.79969, generator loss: 2.51788\n",
            "At epoch: 64/100, after  0 batches have been trained: discriminator loss: 0.71663, generator loss: 1.37496\n",
            "At epoch: 64/100, after 100 batches have been trained: discriminator loss: 0.99478, generator loss: 1.17991\n",
            "At epoch: 64/100, after 200 batches have been trained: discriminator loss: 0.78646, generator loss: 1.36428\n",
            "At epoch: 65/100, after  0 batches have been trained: discriminator loss: 0.89291, generator loss: 2.26218\n",
            "At epoch: 65/100, after 100 batches have been trained: discriminator loss: 0.81959, generator loss: 1.51505\n",
            "At epoch: 65/100, after 200 batches have been trained: discriminator loss: 0.75592, generator loss: 1.46289\n",
            "At epoch: 66/100, after  0 batches have been trained: discriminator loss: 0.84184, generator loss: 1.38889\n",
            "At epoch: 66/100, after 100 batches have been trained: discriminator loss: 0.93808, generator loss: 1.94244\n",
            "At epoch: 66/100, after 200 batches have been trained: discriminator loss: 0.58027, generator loss: 1.67802\n",
            "At epoch: 67/100, after  0 batches have been trained: discriminator loss: 1.21952, generator loss: 1.85911\n",
            "At epoch: 67/100, after 100 batches have been trained: discriminator loss: 0.74321, generator loss: 1.18973\n",
            "At epoch: 67/100, after 200 batches have been trained: discriminator loss: 1.05274, generator loss: 1.14786\n",
            "At epoch: 68/100, after  0 batches have been trained: discriminator loss: 0.98937, generator loss: 2.11430\n",
            "At epoch: 68/100, after 100 batches have been trained: discriminator loss: 0.54205, generator loss: 2.11852\n",
            "At epoch: 68/100, after 200 batches have been trained: discriminator loss: 0.77345, generator loss: 1.46289\n",
            "At epoch: 69/100, after  0 batches have been trained: discriminator loss: 0.74007, generator loss: 1.74101\n",
            "At epoch: 69/100, after 100 batches have been trained: discriminator loss: 0.91342, generator loss: 2.62977\n",
            "At epoch: 69/100, after 200 batches have been trained: discriminator loss: 0.69718, generator loss: 1.92732\n",
            "At epoch: 70/100, after  0 batches have been trained: discriminator loss: 0.57817, generator loss: 1.70601\n",
            "At epoch: 70/100, after 100 batches have been trained: discriminator loss: 0.69566, generator loss: 1.87212\n",
            "At epoch: 70/100, after 200 batches have been trained: discriminator loss: 1.15095, generator loss: 0.96388\n",
            "At epoch: 71/100, after  0 batches have been trained: discriminator loss: 0.84908, generator loss: 1.65258\n",
            "At epoch: 71/100, after 100 batches have been trained: discriminator loss: 0.93887, generator loss: 1.03590\n",
            "At epoch: 71/100, after 200 batches have been trained: discriminator loss: 0.86022, generator loss: 1.65448\n",
            "At epoch: 72/100, after  0 batches have been trained: discriminator loss: 0.69952, generator loss: 1.69643\n",
            "At epoch: 72/100, after 100 batches have been trained: discriminator loss: 0.85283, generator loss: 1.10223\n",
            "At epoch: 72/100, after 200 batches have been trained: discriminator loss: 0.67693, generator loss: 2.23641\n",
            "At epoch: 73/100, after  0 batches have been trained: discriminator loss: 0.62850, generator loss: 1.96243\n",
            "At epoch: 73/100, after 100 batches have been trained: discriminator loss: 0.70950, generator loss: 1.74164\n",
            "At epoch: 73/100, after 200 batches have been trained: discriminator loss: 0.76970, generator loss: 1.66207\n",
            "At epoch: 74/100, after  0 batches have been trained: discriminator loss: 0.57706, generator loss: 1.88780\n",
            "At epoch: 74/100, after 100 batches have been trained: discriminator loss: 0.82046, generator loss: 2.09259\n",
            "At epoch: 74/100, after 200 batches have been trained: discriminator loss: 0.88316, generator loss: 1.88281\n",
            "At epoch: 75/100, after  0 batches have been trained: discriminator loss: 0.82102, generator loss: 2.53255\n",
            "At epoch: 75/100, after 100 batches have been trained: discriminator loss: 0.57182, generator loss: 1.97712\n",
            "At epoch: 75/100, after 200 batches have been trained: discriminator loss: 0.74457, generator loss: 2.32694\n",
            "At epoch: 76/100, after  0 batches have been trained: discriminator loss: 1.02094, generator loss: 1.90386\n",
            "At epoch: 76/100, after 100 batches have been trained: discriminator loss: 0.97941, generator loss: 1.47621\n",
            "At epoch: 76/100, after 200 batches have been trained: discriminator loss: 0.93359, generator loss: 1.26566\n",
            "At epoch: 77/100, after  0 batches have been trained: discriminator loss: 0.72325, generator loss: 1.42659\n",
            "At epoch: 77/100, after 100 batches have been trained: discriminator loss: 0.77781, generator loss: 2.48311\n",
            "At epoch: 77/100, after 200 batches have been trained: discriminator loss: 1.00331, generator loss: 1.43239\n",
            "At epoch: 78/100, after  0 batches have been trained: discriminator loss: 0.88110, generator loss: 1.55074\n",
            "At epoch: 78/100, after 100 batches have been trained: discriminator loss: 0.83560, generator loss: 2.21654\n",
            "At epoch: 78/100, after 200 batches have been trained: discriminator loss: 0.74700, generator loss: 2.89715\n",
            "At epoch: 79/100, after  0 batches have been trained: discriminator loss: 0.70284, generator loss: 2.54748\n",
            "At epoch: 79/100, after 100 batches have been trained: discriminator loss: 0.62475, generator loss: 1.65097\n",
            "At epoch: 79/100, after 200 batches have been trained: discriminator loss: 0.86235, generator loss: 1.96524\n",
            "At epoch: 80/100, after  0 batches have been trained: discriminator loss: 0.97631, generator loss: 1.80334\n",
            "At epoch: 80/100, after 100 batches have been trained: discriminator loss: 0.76087, generator loss: 1.77158\n",
            "At epoch: 80/100, after 200 batches have been trained: discriminator loss: 0.97211, generator loss: 0.94524\n",
            "At epoch: 81/100, after  0 batches have been trained: discriminator loss: 0.91823, generator loss: 1.74819\n",
            "At epoch: 81/100, after 100 batches have been trained: discriminator loss: 0.89480, generator loss: 1.46688\n",
            "At epoch: 81/100, after 200 batches have been trained: discriminator loss: 0.89830, generator loss: 1.83584\n",
            "At epoch: 82/100, after  0 batches have been trained: discriminator loss: 0.83537, generator loss: 2.55192\n",
            "At epoch: 82/100, after 100 batches have been trained: discriminator loss: 1.20364, generator loss: 1.16156\n",
            "At epoch: 82/100, after 200 batches have been trained: discriminator loss: 0.81224, generator loss: 1.29513\n",
            "At epoch: 83/100, after  0 batches have been trained: discriminator loss: 0.73322, generator loss: 2.31725\n",
            "At epoch: 83/100, after 100 batches have been trained: discriminator loss: 0.74091, generator loss: 1.59643\n",
            "At epoch: 83/100, after 200 batches have been trained: discriminator loss: 0.87192, generator loss: 1.83579\n",
            "At epoch: 84/100, after  0 batches have been trained: discriminator loss: 1.01558, generator loss: 2.07593\n",
            "At epoch: 84/100, after 100 batches have been trained: discriminator loss: 0.75771, generator loss: 2.10114\n",
            "At epoch: 84/100, after 200 batches have been trained: discriminator loss: 0.69634, generator loss: 1.01484\n",
            "At epoch: 85/100, after  0 batches have been trained: discriminator loss: 0.86972, generator loss: 1.74920\n",
            "At epoch: 85/100, after 100 batches have been trained: discriminator loss: 0.91671, generator loss: 1.56694\n",
            "At epoch: 85/100, after 200 batches have been trained: discriminator loss: 0.95468, generator loss: 1.78629\n",
            "At epoch: 86/100, after  0 batches have been trained: discriminator loss: 1.13359, generator loss: 1.77018\n",
            "At epoch: 86/100, after 100 batches have been trained: discriminator loss: 0.81194, generator loss: 2.17972\n",
            "At epoch: 86/100, after 200 batches have been trained: discriminator loss: 0.85089, generator loss: 1.68059\n",
            "At epoch: 87/100, after  0 batches have been trained: discriminator loss: 0.60620, generator loss: 1.96925\n",
            "At epoch: 87/100, after 100 batches have been trained: discriminator loss: 0.90450, generator loss: 2.01156\n",
            "At epoch: 87/100, after 200 batches have been trained: discriminator loss: 0.60700, generator loss: 1.87600\n",
            "At epoch: 88/100, after  0 batches have been trained: discriminator loss: 1.08242, generator loss: 0.80395\n",
            "At epoch: 88/100, after 100 batches have been trained: discriminator loss: 0.86262, generator loss: 2.48230\n",
            "At epoch: 88/100, after 200 batches have been trained: discriminator loss: 0.68396, generator loss: 1.86373\n",
            "At epoch: 89/100, after  0 batches have been trained: discriminator loss: 0.88891, generator loss: 2.26177\n",
            "At epoch: 89/100, after 100 batches have been trained: discriminator loss: 0.73249, generator loss: 2.44101\n",
            "At epoch: 89/100, after 200 batches have been trained: discriminator loss: 0.97656, generator loss: 1.70770\n",
            "At epoch: 90/100, after  0 batches have been trained: discriminator loss: 0.64803, generator loss: 1.50882\n",
            "At epoch: 90/100, after 100 batches have been trained: discriminator loss: 1.04628, generator loss: 2.56575\n",
            "At epoch: 90/100, after 200 batches have been trained: discriminator loss: 0.92091, generator loss: 1.61408\n",
            "At epoch: 91/100, after  0 batches have been trained: discriminator loss: 0.95569, generator loss: 2.32950\n",
            "At epoch: 91/100, after 100 batches have been trained: discriminator loss: 0.97306, generator loss: 1.32799\n",
            "At epoch: 91/100, after 200 batches have been trained: discriminator loss: 0.92945, generator loss: 2.14654\n",
            "At epoch: 92/100, after  0 batches have been trained: discriminator loss: 0.74896, generator loss: 2.70390\n",
            "At epoch: 92/100, after 100 batches have been trained: discriminator loss: 0.73748, generator loss: 1.10836\n",
            "At epoch: 92/100, after 200 batches have been trained: discriminator loss: 0.86824, generator loss: 0.92353\n",
            "At epoch: 93/100, after  0 batches have been trained: discriminator loss: 0.84452, generator loss: 2.03244\n",
            "At epoch: 93/100, after 100 batches have been trained: discriminator loss: 0.75882, generator loss: 1.67153\n",
            "At epoch: 93/100, after 200 batches have been trained: discriminator loss: 1.09334, generator loss: 1.38482\n",
            "At epoch: 94/100, after  0 batches have been trained: discriminator loss: 0.88109, generator loss: 1.19781\n",
            "At epoch: 94/100, after 100 batches have been trained: discriminator loss: 0.70337, generator loss: 1.71879\n",
            "At epoch: 94/100, after 200 batches have been trained: discriminator loss: 0.82180, generator loss: 0.99590\n",
            "At epoch: 95/100, after  0 batches have been trained: discriminator loss: 0.66912, generator loss: 2.49435\n",
            "At epoch: 95/100, after 100 batches have been trained: discriminator loss: 0.74199, generator loss: 1.76273\n",
            "At epoch: 95/100, after 200 batches have been trained: discriminator loss: 0.65646, generator loss: 1.65178\n",
            "At epoch: 96/100, after  0 batches have been trained: discriminator loss: 0.61735, generator loss: 2.54837\n",
            "At epoch: 96/100, after 100 batches have been trained: discriminator loss: 0.73803, generator loss: 2.10564\n",
            "At epoch: 96/100, after 200 batches have been trained: discriminator loss: 0.81280, generator loss: 2.19097\n",
            "At epoch: 97/100, after  0 batches have been trained: discriminator loss: 0.67510, generator loss: 2.27100\n",
            "At epoch: 97/100, after 100 batches have been trained: discriminator loss: 0.57277, generator loss: 2.08299\n",
            "At epoch: 97/100, after 200 batches have been trained: discriminator loss: 0.69060, generator loss: 1.72117\n",
            "At epoch: 98/100, after  0 batches have been trained: discriminator loss: 1.26620, generator loss: 1.88917\n",
            "At epoch: 98/100, after 100 batches have been trained: discriminator loss: 0.97393, generator loss: 2.04116\n",
            "At epoch: 98/100, after 200 batches have been trained: discriminator loss: 0.60868, generator loss: 1.51684\n",
            "At epoch: 99/100, after  0 batches have been trained: discriminator loss: 1.03833, generator loss: 1.40062\n",
            "At epoch: 99/100, after 100 batches have been trained: discriminator loss: 0.58847, generator loss: 1.77324\n",
            "At epoch: 99/100, after 200 batches have been trained: discriminator loss: 0.83384, generator loss: 2.18849\n",
            "At epoch: 100/100, after  0 batches have been trained: discriminator loss: 0.83180, generator loss: 2.28195\n",
            "At epoch: 100/100, after 100 batches have been trained: discriminator loss: 1.03345, generator loss: 1.36400\n",
            "At epoch: 100/100, after 200 batches have been trained: discriminator loss: 0.63065, generator loss: 2.86150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "## Step 5: Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NywiH3nL8guF"
      },
      "source": [
        "### Greate GIFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGKQgENQ8lEI",
        "colab": {}
      },
      "source": [
        "# set the path of the GIF file\n",
        "anime_path = os.path.join(cur_image_dir, \"%s-%s.%s\" % (structure, dataset, 'gif'))\n",
        "# create the GIF with imageio\n",
        "with imageio.get_writer(anime_path, mode='I') as writer:\n",
        "    filenames = glob.glob(os.path.join(cur_image_dir, '*.png'))\n",
        "    filenames = sorted(filenames)\n",
        "    last = -1\n",
        "    for i,filename in enumerate(filenames):\n",
        "        frame = 2*(i**0.5)\n",
        "        if round(frame) > round(last):\n",
        "            last = frame\n",
        "        else:\n",
        "            continue\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "# show the GIF in the notebook\n",
        "# display.Image(filename=anime_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9XNueB23DgHs"
      },
      "source": [
        "### Tensor Board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5UdH3Mt2DgHs",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir=log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}