{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rF2x3qooyBTI"
   },
   "source": [
    "# Deep Convolutional Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGM8xKZrDgHD"
   },
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0uoZZ7fDgHE"
   },
   "source": [
    "### Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWGkAtMXDgHE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"%d GPUs are available for tensorflow %s in current environment.\" % \n",
    "      (len(tf.config.experimental.list_physical_devices('GPU')), tf.__version__))\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsEAA1m5DgHG"
   },
   "source": [
    "### Set the Paths of the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TouXUserDgHH"
   },
   "outputs": [],
   "source": [
    "# get the dictionary for the project\n",
    "pwd = os.path.dirname(os.getcwd())\n",
    "# set and create the path for log file for tesnorboard\n",
    "log_dir = os.path.join(pwd, 'outputs', 'logs')\n",
    "os.makedirs(log_dir, exist_ok = True)\n",
    "# set and create the path for saving the images\n",
    "image_dir = os.path.join(pwd, 'outputs', 'images')\n",
    "os.makedirs(image_dir, exist_ok = True)\n",
    "# set and create the path for saving the weights of the model\n",
    "checkpoint_dir = os.path.join(pwd, 'outputs', 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZC_fHe_DgHJ"
   },
   "outputs": [],
   "source": [
    "# config the version of training, structure of model and usage of dataset\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "structure = 'WGAN'\n",
    "dataset = 'MNIST'\n",
    "config = \"%s-%s-%s\" % (structure, dataset, stamp)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "(mnist_train, _), (mnist_val, _) = tf.keras.datasets.mnist.load_data()\n",
    "# load svhn data\n",
    "svhn_train = tfds.load(\"svhn_cropped\", with_info=False, as_supervised=True, split=tfds.Split.TRAIN)\n",
    "svhn_val = tfds.load(\"svhn_cropped\", with_info=False, as_supervised=True, split=tfds.Split.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11DxF_d8DgHM"
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def mnist_preprocess(x):        \n",
    "    x = 2*tf.cast(x, dtype=tf.float32)/255 - 1\n",
    "    # expand the 3d tensor to 4d\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def svhn_preprocess(x, y):        \n",
    "    x = 2*tf.cast(x, dtype=tf.float32)/255 - 1      \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "# preprocess the MNIST dataset\n",
    "mnist_train_db = tf.data.Dataset.from_tensor_slices(mnist_train).map(mnist_preprocess).shuffle(60000)\n",
    "mnist_val_db = tf.data.Dataset.from_tensor_slices(mnist_val).map(mnist_preprocess)\n",
    "# get one batch and check the dimension of this batch\n",
    "mnist_samples = next(iter(mnist_val_db.batch(32)))\n",
    "print(\"shape of one batch for MNIST images is: \" + str(mnist_samples.shape))\n",
    "# preprocess the SVHN dataset\n",
    "svhn_train_db = svhn_train.map(svhn_preprocess).shuffle(60000)\n",
    "svhn_val_db = svhn_val.map(svhn_preprocess)\n",
    "svhn_samples = next(iter(svhn_val_db.batch(32)))\n",
    "print(\"shape of one batch for SVHN images is: \" + str(svhn_samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Step 2: Define the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0CnPpMfDgHS"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, size, channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc0 = layers.Dense(size*size*256, use_bias=False)\n",
    "        self.bn0 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3 = layers.Conv2DTranspose(filters=channel, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
    "\n",
    "        self.size = size\n",
    "         \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc0(inputs)   \n",
    "        x = tf.reshape(x, [-1, self.size, self.size, 256]) \n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        \n",
    "        image = tf.math.tanh(self.conv3(x))\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZK5odTrNDgHT"
   },
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    generator = Generator(size = 7, channel = 1)\n",
    "elif dataset == 'SVHN':\n",
    "    generator = Generator(size = 8, channel = 3)\n",
    "seed = tf.random.normal([32, 100])\n",
    "image_fake = generator(seed, training=False)\n",
    "print(\"The shape of the output of the generator is: \" + str(image_fake.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHakGKzeDgHU"
   },
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    plt.imshow(image_fake[0, :, :, 0], cmap='gray')\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.imshow(image_fake[0, :, :, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWx887DtDgHW"
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same')\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(filters=256, kernel_size=5, strides=2, padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout3 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = tf.nn.leaky_relu(self.dropout1(self.conv1(inputs)))\n",
    "        x = self.dropout2(tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training)))\n",
    "        x = self.dropout3(tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training)))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wExWVnv0DgHY"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "logits = discriminator(image_fake, training=False)\n",
    "print(\"The shape of logits given by discriminator is: \" + str(logits.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Step 3: Define some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPhPSHepDgHa"
   },
   "outputs": [],
   "source": [
    "loss_cal = tf.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Penalty Calculator for WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_cal(discriminator, image_fake, image_true):\n",
    "    \n",
    "    # [b, 1, 1, 1] -> [b, h, w, c]\n",
    "    batch = image_true.shape[0]\n",
    "    alpha = tf.random.uniform([batch, 1, 1, 1])\n",
    "    alpha = tf.broadcast_to(alpha, image_true.shape)\n",
    "    \n",
    "    interplate = alpha * image_true + (1-alpha)*image_fake\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interplate)\n",
    "        interplate_logits = discriminator(interplate)\n",
    "    grads = tape.gradient(interplate_logits, interplate)\n",
    "    \n",
    "    # [b, h, w, c] -> [b, h*w*c] -> [b]\n",
    "    grads = tf.reshape(grads, [batch, -1])\n",
    "    norm = tf.norm(grads, axis=1)\n",
    "    \n",
    "    grads_penalty = tf.reduce_mean(tf.math.squared_difference(norm, 1))\n",
    "    \n",
    "    return grads_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Discriminator Loss Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss_cal(generator, discriminator, seed, image_true, training, LAMBDA=1.):\n",
    "    \n",
    "    image_fake = generator(seed, training)\n",
    "    \n",
    "    # treat real image as real\n",
    "    logits_real = discriminator(image_true, training)\n",
    "    loss_real = tf.reduce_mean(loss_cal(tf.ones_like(logits_real), logits_real))\n",
    "    # treat fake image as fake\n",
    "    logits_fake = discriminator(image_fake, training)\n",
    "    loss_fake = tf.reduce_mean(loss_cal(tf.zeros_like(logits_fake), logits_fake)) \n",
    "    \n",
    "    loss = loss_fake + loss_real\n",
    "    if structure == 'GAN':\n",
    "        return loss\n",
    "    elif structure == 'WGAN':\n",
    "        gradient_penalty = gradient_penalty_cal(discriminator, image_true, image_fake)\n",
    "        loss += LAMBDA * gradient_penalty\n",
    "    \n",
    "    return (loss, gradient_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Generator Loss Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O54tCwCzDgHe"
   },
   "outputs": [],
   "source": [
    "def generator_loss_cal(generator, discriminator, seed, training):\n",
    "    \n",
    "    image_fake = generator(seed, training)\n",
    "    logits_fake = discriminator(image_fake, training)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss_cal(tf.ones_like(logits_fake), logits_fake))     \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9S9hxTnDgHf"
   },
   "source": [
    "### Image Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSmdsjHODgHg"
   },
   "outputs": [],
   "source": [
    "def save_image(images, height, width, image_path, index):\n",
    "\n",
    "    images = ((images + 1.0) * 127.5).astype(np.uint8)\n",
    "    # final picture which save as PNG\n",
    "    picture = np.array([])\n",
    "    \n",
    "    row = np.array([])\n",
    "    for i in range(height*width):\n",
    "        # concat image into a row\n",
    "        if row.size == 0:\n",
    "            row = images[i]\n",
    "        else:\n",
    "            row = np.concatenate((row, images[i]), axis=1)\n",
    "\n",
    "        # concat image row to picture\n",
    "        if (i+1) % width == 0:\n",
    "            if picture.size == 0:\n",
    "                picture = row\n",
    "            else:\n",
    "                picture = np.concatenate((picture, row), axis=0)\n",
    "            # reset single row\n",
    "            row = np.array([])\n",
    "    \n",
    "    picture_board = tf.expand_dims(tf.cast(picture, dtype=tf.uint8), 0)\n",
    "    with summary_writer.as_default(): \n",
    "        tf.summary.image('train-fake-images:', picture_board, step=index)\n",
    "    \n",
    "    if 'MNIST' in image_path:\n",
    "        picture = np.squeeze(picture, axis=2)\n",
    "        Image.fromarray(picture, 'P').save(image_path)\n",
    "    elif 'SVHN' in image_path:\n",
    "        Image.fromarray(picture, 'RGB').save(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Step 4: Define the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2MVkQZmDgHh"
   },
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "EPOCH = 20\n",
    "BATCH = 128\n",
    "# step size, i.e. number of batches per validation\n",
    "STEP = 10\n",
    "SAVE = 10\n",
    "\n",
    "SEED = 100\n",
    "LAMBDA = 1.\n",
    "LEARNING_RATE_D = 1e-4\n",
    "LEARNING_RATE_G = 1e-3\n",
    "\n",
    "RESTORE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA1gbMUGDgHl"
   },
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    train_db = mnist_train_db.batch(BATCH, drop_remainder=True)\n",
    "    val_db = mnist_val_db.batch(BATCH, drop_remainder=True)\n",
    "elif dataset == 'SVHN':\n",
    "    train_db = svhn_train_db.batch(BATCH, drop_remainder=True)\n",
    "    val_db = svhn_val_db.batch(BATCH, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE_G)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "# save the training process\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, config, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# the log file created for tensorborad for this time of training\n",
    "cur_log_dir = os.path.join(log_dir, config)\n",
    "summary_writer = tf.summary.create_file_writer(cur_log_dir) \n",
    "\n",
    "# the file for saving images for this time of training\n",
    "cur_image_dir = os.path.join(image_dir, config)\n",
    "os.makedirs(cur_image_dir, exist_ok = True)\n",
    "\n",
    "# if you want to see the visualization during the traing, run this line\n",
    "# %tensorboard --logdir=log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBPZljjBDgHo"
   },
   "outputs": [],
   "source": [
    "# restore the pretrained model\n",
    "if RESTORE:\n",
    "    restore_checkpoint_dir = os.path.join(checkpoint_dir, restore)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(restore_checkpoint_dir))\n",
    "    \n",
    "# number of batches have trained\n",
    "index = -1\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    \n",
    "    for step, image_true in enumerate(train_db):\n",
    "        \n",
    "        seed = tf.random.uniform([BATCH, SEED], minval=-1, maxval=1)\n",
    "        # train Discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            if structure == 'GAN':\n",
    "                discriminator_loss = discriminator_loss_cal(generator, discriminator, seed, image_true, training=True)\n",
    "            elif structure == 'WGAN':\n",
    "                discriminator_loss, gradient_penalty = discriminator_loss_cal(generator, discriminator, seed, image_true, training=True)\n",
    "        grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
    "        discriminator_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "        \n",
    "        # train Generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            generator_loss = generator_loss_cal(generator, discriminator, seed, training=True)\n",
    "        grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "        generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))     \n",
    "        \n",
    "        if step % STEP == 0:\n",
    "            index += 1\n",
    "            print(\"At epoch: %s/%d, after %s batches have been trained: discriminator loss: %.5f, generator loss: %.5f\" % \n",
    "                  (format(epoch, '2'), EPOCH, format(step,'2'), float(discriminator_loss), float(generator_loss)))\n",
    "            print(gradient_penalty)\n",
    "            \n",
    "            with summary_writer.as_default(): \n",
    "                tf.summary.scalar('train-generator-loss', float(generator_loss), step=index) \n",
    "                tf.summary.scalar('train-discriminator-loss', float(discriminator_loss), step=index) \n",
    "                \n",
    "            fake_image = generator(seed, training=False)\n",
    "            image_path = os.path.join(cur_image_dir, 'after-%d-batches.png' % index)\n",
    "            save_image(fake_image.numpy(), 5, 5, image_path, index)\n",
    "    \n",
    "    if epoch % SAVE == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Step 5: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NywiH3nL8guF"
   },
   "source": [
    "### Greate GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "# set the path of the GIF file\n",
    "anime_path = os.path.join(cur_image_dir, \"%s-%s.%s\" % (structure, dataset, 'gif'))\n",
    "# create the GIF with imageio\n",
    "with imageio.get_writer(anime_path, mode='I') as writer:\n",
    "    filenames = glob.glob(os.path.join(cur_image_dir, '*.png'))\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "# show the GIF in the notebook\n",
    "display.Image(filename=anime_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XNueB23DgHs"
   },
   "source": [
    "### Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UdH3Mt2DgHs"
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=log_dir"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gan_mnist_local.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
